{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h1><div align=\"center\">Managing Accelerated Application Memory with CUDA C/C++ Unified Memory</div></h1>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![CUDA](./images/CUDA_Logo.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The [*CUDA Best Practices Guide*](http://docs.nvidia.com/cuda/cuda-c-best-practices-guide/index.html#memory-optimizations), a highly recommended followup to this and other CUDA fundamentals labs, recommends a design cycle called **APOD**: **A**ssess, **P**arallelize, **O**ptimize, **D**eploy. In short, APOD prescribes an iterative design process, where developers can apply incremental improvements to their accelerated application's performance, and ship their code. As developers become more competent CUDA programmers, more advanced optimization techniques can be applied to their accelerated code bases.\n",
    "\n",
    "This lab will support such a style of iterative development. You will be using the Nsight Systems command line tool **nsys** to qualitatively measure your application's performance, and to identify opportunities for optimization, after which you will apply incremental improvements before learning new techniques and repeating the cycle. As a point of focus, many of the techniques you will be learning and applying in this lab will deal with the specifics of how CUDA's **Unified Memory** works. Understanding Unified Memory behavior is a fundamental skill for CUDA developers, and serves as a prerequisite to many more advanced memory management techniques."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Prerequisites\n",
    "\n",
    "To get the most out of this lab you should already be able to:\n",
    "\n",
    "- Write, compile, and run C/C++ programs that both call CPU functions and launch GPU kernels.\n",
    "- Control parallel thread hierarchy using execution configuration.\n",
    "- Refactor serial loops to execute their iterations in parallel on a GPU.\n",
    "- Allocate and free Unified Memory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Objectives\n",
    "\n",
    "By the time you complete this lab, you will be able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Iterative Optimizations with the NVIDIA Command Line Profiler\n",
    "\n",
    "The only way to be assured that attempts at optimizing accelerated code bases are actually successful is to profile the application for quantitative information about the application's performance. `nsys` is the Nsight Systems command line tool. It ships with the CUDA toolkit, and is a powerful tool for profiling accelerated applications.\n",
    "\n",
    "`nsys` is easy to use. Its most basic usage is to simply pass it the path to an executable compiled with `nvcc`. `nsys` will proceed to execute the application, after which it will print a summary output of the application's GPU activities, CUDA API calls, as well as information about **Unified Memory** activity, a topic which will be covered extensively later in this lab.\n",
    "\n",
    "When accelerating applications, or optimizing already-accelerated applications, take a scientific and iterative approach. Profile your application after making changes, take note, and record the implications of any refactoring on performance. Make these observations early and often: frequently, enough performance boost can be gained with little effort such that you can ship your accelerated application. Additionally, frequent profiling will teach you how specific changes to your CUDA code bases impact its actual performance: knowledge that is hard to acquire when only profiling after many kinds of changes in your code bases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Profile an Application with nsys\n",
    "\n",
    "[01-vector-add.cu](01-vector-add/01-vector-add.cu) (<------ you can click on this and any of the source file links in this lab to open them for editing) is a naively accelerated vector addition program. Use the two code execution cells below (`CTRL` + `ENTER`). The first code execution cell will compile (and run) the vector addition program. The second code execution cell will profile the executable that was just compiled using `nsys profile`.\n",
    "\n",
    "`nsys profile` will generate a report file which can be used in a variety of manners, including for use in visual profiling with Nsight Systems, which we will look at in more detail in the following section.\n",
    "\n",
    "Here we use the `--stats=true` flag to indicate we would like summary statistics printed. In this section this summary will be the focus of our attention. There is quite a lot of information printed:\n",
    "\n",
    "- Operating System Runtime Summary (`osrt_sum`)\n",
    "- **CUDA API Summary (`cuda_api_sum`)**\n",
    "- **CUDA Kernel Summary (`cuda_gpu_kern_sum`)**\n",
    "- **CUDA Memory Time Operation Summary (`cuda_gpu_mem_time_sum`)**\n",
    "- **CUDA Memory Size Operation Summary (`cuda_gpu_mem_size_sum`)**\n",
    "\n",
    "In this section you will primarily be using the 4 summaries in **bold** above. In the next section, you will be using the generated report files to give to the Nsight Systems GUI for visual profiling."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After profiling the application, answer the following questions using information displayed in the `cuda_gpu_kern_sum` section of the profiling output:\n",
    "\n",
    "- What was the name of the only CUDA kernel called in this application?\n",
    "- How many times did this kernel run?\n",
    "- How long did it take this kernel to run? Record this time somewhere: you will be optimizing this application and will want to know how much faster you can make it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o single-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-d350.qdstrm'\n",
      "[1/8] [========================100%] report1.nsys-rep\n",
      "[2/8] [========================100%] report1.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report1.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     90.4       6175168421        320  19297401.3  10073339.0     26980  100167209   27563853.4  poll                  \n",
      "      8.7        597457826        285   2096343.2   2064370.0       200   20437248    1325442.1  sem_timedwait         \n",
      "      0.5         37193868        499     74536.8     12770.0       390    9229559     441077.0  ioctl                 \n",
      "      0.3         19239157         24    801631.5      5080.0      1160    7197340    2159030.5  mmap                  \n",
      "      0.0          1106579         27     40984.4      4600.0      3520     715200     135669.8  mmap64                \n",
      "      0.0           527870         44     11997.0     10900.0      4190      39160       6549.8  open64                \n",
      "      0.0           179580          4     44895.0     43775.0     36660      55370       8766.4  pthread_create        \n",
      "      0.0           159240         29      5491.0      3520.0      1630      36800       6778.4  fopen                 \n",
      "      0.0           148040         11     13458.2     11960.0      9650      19260       2965.4  write                 \n",
      "      0.0            57660         26      2217.7        90.0        70      55450      10857.3  fgets                 \n",
      "      0.0            55270         11      5024.5      3520.0      1480      20470       5235.6  munmap                \n",
      "      0.0            37080          6      6180.0      5400.0      3310       9550       2257.5  open                  \n",
      "      0.0            31760         52       610.8       480.0       210       4970        661.9  fcntl                 \n",
      "      0.0            26910         22      1223.2      1020.0       710       2890        499.5  fclose                \n",
      "      0.0            20250         14      1446.4      1260.0       450       3530        877.7  read                  \n",
      "      0.0            15050          2      7525.0      7525.0      3640      11410       5494.2  socket                \n",
      "      0.0            11000          5      2200.0      1210.0        70       6480       2676.5  fread                 \n",
      "      0.0            10180          1     10180.0     10180.0     10180      10180          0.0  connect               \n",
      "      0.0             7930          1      7930.0      7930.0      7930       7930          0.0  pipe2                 \n",
      "      0.0             7150         64       111.7       130.0        40        440         80.4  pthread_mutex_trylock \n",
      "      0.0             2230          1      2230.0      2230.0      2230       2230          0.0  bind                  \n",
      "      0.0             1440          1      1440.0      1440.0      1440       1440          0.0  listen                \n",
      "      0.0              290          1       290.0       290.0       290        290          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ---------------------\n",
      "     94.6       2500196211          1  2500196211.0  2500196211.0  2500196211  2500196211          0.0  cudaDeviceSynchronize\n",
      "      4.7        123988347          3    41329449.0       28620.0       14430   123945297   71547423.5  cudaMallocManaged    \n",
      "      0.7         19262168          3     6420722.7     6188529.0     5844999     7228640     720452.0  cudaFree             \n",
      "      0.0            46830          1       46830.0       46830.0       46830       46830          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances    Avg (ns)      Med (ns)     Min (ns)    Max (ns)   StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ------------  ------------  ----------  ----------  -----------  ----------------------------------------------\n",
      "    100.0       2500187016          1  2500187016.0  2500187016.0  2500187016  2500187016          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.5         34146762   2304   14820.6    4367.5      1983     80223      22489.3  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.5         11059281    768   14400.1    3743.5      1279     80767      22788.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report1.nsys-rep\n",
      "    /dli/task/report1.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./single-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Worth mentioning is that by default, `nsys profile` will not overwrite an existing report file. This is done to prevent accidental loss of work when profiling. If for any reason, you would rather overwrite an existing report file, say during rapid iterations, you can provide the `-f` flag to `nsys profile` to allow overwriting an existing report file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize and Profile\n",
    "\n",
    "Take a minute or two to make a simple optimization to [01-vector-add.cu](01-vector-add/01-vector-add.cu) by updating its execution configuration so that it runs on many threads in a single thread block. Recompile and then profile with `nsys profile --stats=true` using the code execution cells below. Use the profiling output to check the runtime of the kernel. What was the speed up from this optimization? Be sure to record your results somewhere."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o multi-thread-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-e203.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.9       1767535725         97  18222017.8  10077199.0      3100  100151164   26762126.6  poll                  \n",
      "      9.3        187361946         86   2178627.3   2071333.5       110   20554581    2552169.7  sem_timedwait         \n",
      "      1.7         35071684        497     70566.8     11801.0       370    8785969     422982.9  ioctl                 \n",
      "      1.0         19312648         24    804693.7      6740.5      1180    7213431    2164045.3  mmap                  \n",
      "      0.1          1158917         27     42922.9      4970.0      3230     762673     144586.6  mmap64                \n",
      "      0.0           602067         44     13683.3     12110.5      4420      37721       6797.4  open64                \n",
      "      0.0           201387          4     50346.8     50596.5     44202      55992       5799.0  pthread_create        \n",
      "      0.0           191400         29      6600.0      3980.0      1511      28481       6489.6  fopen                 \n",
      "      0.0           155174         11     14106.7     14110.0      1110      22681       5463.7  write                 \n",
      "      0.0            80470         10      8047.0      5845.0      1790      21160       6198.3  munmap                \n",
      "      0.0            55611         26      2138.9        80.0        70      53511      10477.9  fgets                 \n",
      "      0.0            47003          6      7833.8      8806.0      3590      10290       2755.2  open                  \n",
      "      0.0            41171         52       791.8       555.0       240       6580        911.0  fcntl                 \n",
      "      0.0            33530         22      1524.1      1255.0       640       4340        868.8  fclose                \n",
      "      0.0            26721         14      1908.6      1370.0       390       6290       1829.8  read                  \n",
      "      0.0            23241          2     11620.5     11620.5      8120      15121       4950.5  socket                \n",
      "      0.0            13691          1     13691.0     13691.0     13691      13691          0.0  connect               \n",
      "      0.0             9271          5      1854.2      1400.0        70       4840       1988.2  fread                 \n",
      "      0.0             7270          1      7270.0      7270.0      7270       7270          0.0  pipe2                 \n",
      "      0.0             6490         64       101.4       130.0        40        440         63.6  pthread_mutex_trylock \n",
      "      0.0             2990          1      2990.0      2990.0      2990       2990          0.0  bind                  \n",
      "      0.0             1430          1      1430.0      1430.0      1430       1430          0.0  listen                \n",
      "      0.0              200          1       200.0       200.0       200        200          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     52.9        123922864          3  41307621.3     77223.0     21550  123824091   71461364.4  cudaMallocManaged    \n",
      "     38.8         91032334          1  91032334.0  91032334.0  91032334   91032334          0.0  cudaDeviceSynchronize\n",
      "      8.3         19349853          3   6449951.0   6166249.0   5925351    7258253     710297.2  cudaFree             \n",
      "      0.0            49722          1     49722.0     49722.0     49722      49722          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         91021951          1  91021951.0  91021951.0  91021951  91021951          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.3         33725033   2304   14637.6    4047.5      1792     80225      22499.9  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.7         11061238    768   14402.7    3711.5      1343     80930      22793.8  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2304     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./multi-thread-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Iteratively\n",
    "\n",
    "In this exercise you will go through several cycles of editing the execution configuration of [01-vector-add.cu](01-vector-add/01-vector-add.cu), profiling it, and recording the results to see the impact. Use the following guidelines while working:\n",
    "\n",
    "- Start by listing 3 to 5 different ways you will update the execution configuration, being sure to cover a range of different grid and block size combinations.\n",
    "- Edit the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program in one of the ways you listed.\n",
    "- Compile and profile your updated code with the two code execution cells below.\n",
    "- Record the runtime of the kernel execution, as given in the profiling output.\n",
    "- Repeat the edit/profile/record cycle for each possible optimization you listed above\n",
    "\n",
    "Which of the execution configurations you attempted proved to be the fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o iteratively-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-c4dc.qdstrm'\n",
      "[1/8] [========================100%] report5.nsys-rep\n",
      "[2/8] [========================100%] report5.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report5.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     87.8       1767723121         97  18223949.7  10078024.0      2330  100142487   26799529.5  poll                  \n",
      "      9.6        192591715         86   2239438.5   2072865.0       130   20415273    2885504.8  sem_timedwait         \n",
      "      1.6         31258526        497     62894.4     10431.0       490    8062895     384721.6  ioctl                 \n",
      "      1.0         20074067         24    836419.5      7790.0       890    7274914    2248497.4  mmap                  \n",
      "      0.0           915891         27     33921.9      4550.0      2890     550928     104249.7  mmap64                \n",
      "      0.0           498207         44     11322.9     10505.0      3510      35771       5510.3  open64                \n",
      "      0.0           187435         29      6463.3      4250.0      1480      39271       7707.9  fopen                 \n",
      "      0.0           186232          4     46558.0     43750.5     33220      65511      15250.6  pthread_create        \n",
      "      0.0           141503         11     12863.9     13521.0      1030      19791       4801.4  write                 \n",
      "      0.0            84621         11      7692.8      4080.0      2240      28281       8178.4  munmap                \n",
      "      0.0            51001         26      1961.6        70.0        60      49241       9643.2  fgets                 \n",
      "      0.0            39711          6      6618.5      7985.0      2430       8760       2651.1  open                  \n",
      "      0.0            35651         52       685.6       495.0       160       6010        818.8  fcntl                 \n",
      "      0.0            27820         22      1264.5      1075.0       520       3260        662.0  fclose                \n",
      "      0.0            19990         14      1427.9      1205.0       500       4040       1051.3  read                  \n",
      "      0.0            17940          2      8970.0      8970.0      4710      13230       6024.5  socket                \n",
      "      0.0            11480          1     11480.0     11480.0     11480      11480          0.0  connect               \n",
      "      0.0             8300          5      1660.0      1340.0        80       3320       1515.9  fread                 \n",
      "      0.0             6620          1      6620.0      6620.0      6620       6620          0.0  pipe2                 \n",
      "      0.0             5630         64        88.0        50.0        40        180         49.2  pthread_mutex_trylock \n",
      "      0.0             2580          1      2580.0      2580.0      2580       2580          0.0  bind                  \n",
      "      0.0             1390          1      1390.0      1390.0      1390       1390          0.0  listen                \n",
      "      0.0              280          1       280.0       280.0       280        280          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     52.2        109708275          3  36569425.0     46431.0     28740  109633104   63275002.7  cudaMallocManaged    \n",
      "     38.2         80131300          1  80131300.0  80131300.0  80131300   80131300          0.0  cudaDeviceSynchronize\n",
      "      9.6         20095378          3   6698459.3   6848608.0   5932385    7314385     703128.3  cudaFree             \n",
      "      0.0            52971          1     52971.0     52971.0     52971      52971          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         80121892          1  80121892.0  80121892.0  80121892  80121892          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     75.2         33549464   2217   15132.8    6433.0      1823     78689      20333.6  [CUDA Unified Memory memcpy HtoD]\n",
      "     24.8         11069283    768   14413.1    3727.5      1311     99937      22873.7  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653   2217     0.182     0.066     0.004     1.028        0.272  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report5.nsys-rep\n",
      "    /dli/task/report5.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./iteratively-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Streaming Multiprocessors and Querying the Device\n",
    "\n",
    "This section explores how understanding a specific feature of the GPU hardware can promote optimization. After introducing **Streaming Multiprocessors**, you will attempt to further optimize the accelerated vector addition program you have been working on."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_1.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Streaming Multiprocessors and Warps\n",
    "\n",
    "The GPUs that CUDA applications run on have processing units called **streaming multiprocessors**, or **SMs**. During kernel execution, blocks of threads are given to SMs to execute. In order to support the GPU's ability to perform as many parallel operations as possible, performance gains can often be had by *choosing a grid size that has a number of blocks that is a multiple of the number of SMs on a given GPU.*\n",
    "\n",
    "Additionally, SMs create, manage, schedule, and execute groupings of 32 threads from within a block called **warps**. A more [in depth coverage of SMs and warps](http://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#hardware-implementation) is beyond the scope of this course, however, it is important to know that performance gains can also be had by *choosing a block size that has a number of threads that is a multiple of 32.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Programmatically Querying GPU Device Properties\n",
    "\n",
    "In order to support portability, since the number of SMs on a GPU can differ depending on the specific GPU being used, the number of SMs should not be hard-coded into a code bases. Rather, this information should be acquired programatically.\n",
    "\n",
    "The following shows how, in CUDA C/C++, to obtain a C struct which contains many properties about the currently active GPU device, including its number of SMs:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                  // `deviceId` now points to the id of the currently active GPU.\n",
    "\n",
    "cudaDeviceProp props;\n",
    "cudaGetDeviceProperties(&props, deviceId); // `props` now has many useful properties about\n",
    "                                           // the active GPU device.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Query the Device\n",
    "\n",
    "Currently, [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) contains many unassigned variables, and will print gibberish information intended to describe details about the currently active GPU.\n",
    "\n",
    "Build out [01-get-device-properties.cu](04-device-properties/01-get-device-properties.cu) to print the actual values for the desired device properties indicated in the source code. In order to support your work, and as an introduction to them, use the [CUDA Runtime Docs](http://docs.nvidia.com/cuda/cuda-runtime-api/structcudaDeviceProp.html) to help identify the relevant properties in the device props struct. Refer to [the solution](04-device-properties/solutions/01-get-device-properties-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device ID: 0\n",
      "Number of SMs: 80\n",
      "Compute Capability Major: 8\n",
      "Compute Capability Minor: 6\n",
      "Warp Size: 32\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o get-device-properties 04-device-properties/01-get-device-properties.cu -run"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Optimize Vector Add with Grids Sized to Number of SMs\n",
    "\n",
    "Utilize your ability to query the device for its number of SMs to refactor the `addVectorsInto` kernel you have been working on inside [01-vector-add.cu](01-vector-add/01-vector-add.cu) so that it launches with a grid containing a number of blocks that is a multiple of the number of SMs on the device.\n",
    "\n",
    "Depending on other specific details in the code you have written, this refactor may or may not improve, or significantly change, the performance of your kernel. Therefore, as always, be sure to use `nsys profile` so that you can quantitatively evaluate performance changes. Record the results with the rest of your findings thus far, based on the profiling output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o sm-optimized-vector-add 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-f197.qdstrm'\n",
      "[1/8] [========================100%] report4.nsys-rep\n",
      "[2/8] [========================100%] report4.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report4.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     88.0       1781956620         99  17999561.8  10071609.0      2150  100133930   26566075.4  poll                  \n",
      "      9.3        188924087         89   2122742.6   2065938.0       200   20560837    2264984.1  sem_timedwait         \n",
      "      1.6         31862762        497     64110.2     12320.0       410    8151311     387045.6  ioctl                 \n",
      "      1.0         20765527         24    865230.3      4920.0      1030    7423606    2324406.9  mmap                  \n",
      "      0.0           905385         27     33532.8      4120.0      3140     549896     104155.3  mmap64                \n",
      "      0.0           522916         44     11884.5     10800.0      3830      31080       5989.1  open64                \n",
      "      0.0           181459         11     16496.3     14650.0      1140      53509      13185.2  write                 \n",
      "      0.0           181018         29      6242.0      3840.0      1310      37640       7613.7  fopen                 \n",
      "      0.0           165520          4     41380.0     38240.0     31470      57570      11531.0  pthread_create        \n",
      "      0.0            78460         12      6538.3      4355.0      1450      16090       5360.7  munmap                \n",
      "      0.0            51490         26      1980.4        70.0        60      49670       9726.8  fgets                 \n",
      "      0.0            40040          6      6673.3      7670.0      2580       9860       2815.4  open                  \n",
      "      0.0            35180         52       676.5       510.0       160       5430        744.1  fcntl                 \n",
      "      0.0            28070         22      1275.9      1055.0       530       3820        738.1  fclose                \n",
      "      0.0            20930         14      1495.0      1220.0       470       4300       1083.2  read                  \n",
      "      0.0            18590          2      9295.0      9295.0      5260      13330       5706.4  socket                \n",
      "      0.0            10760          1     10760.0     10760.0     10760      10760          0.0  connect               \n",
      "      0.0             8850          1      8850.0      8850.0      8850       8850          0.0  pipe2                 \n",
      "      0.0             8400          5      1680.0      1070.0        60       4520       1884.2  fread                 \n",
      "      0.0             5800         64        90.6        50.0        40        540         75.4  pthread_mutex_trylock \n",
      "      0.0             2480          1      2480.0      2480.0      2480       2480          0.0  bind                  \n",
      "      0.0             1450          1      1450.0      1450.0      1450       1450          0.0  listen                \n",
      "      0.0              290          1       290.0       290.0       290        290          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     48.9        109834752          3  36611584.0     29050.0     17710  109787992   63372628.5  cudaMallocManaged    \n",
      "     41.8         93829158          1  93829158.0  93829158.0  93829158   93829158          0.0  cudaDeviceSynchronize\n",
      "      9.3         20800476          3   6933492.0   6710780.0   6619331    7470365     467188.6  cudaFree             \n",
      "      0.0            44360          1     44360.0     44360.0     44360      44360          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         93818867          1  93818867.0  93818867.0  93818867  93818867          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     82.3         51586363  13569    3801.8    2175.0      1822     77726       7586.8  [CUDA Unified Memory memcpy HtoD]\n",
      "     17.7         11060937    768   14402.3    3743.5      1374     80542      22785.7  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  13569     0.030     0.008     0.004     1.016        0.101  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report4.nsys-rep\n",
      "    /dli/task/report4.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Unified Memory Details\n",
    "\n",
    "You have been allocating memory intended for use either by host or device code with `cudaMallocManaged` and up until now have enjoyed the benefits of this method - automatic memory migration, ease of programming - without diving into the details of how the **Unified Memory** (**UM**) allocated by `cudaMallocManaged` actual works.\n",
    "\n",
    "`nsys profile` provides details about UM management in accelerated applications, and using this information, in conjunction with a more-detailed understanding of how UM works, provides additional opportunities to optimize accelerated applications."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following video presents upcoming material visually, at a high level. Click watch it before moving on to more detailed coverage of their topics in following sections.\n",
    "\n",
    "<script>console.log('hi');</script>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<video controls width=\"640\" height=\"360\">\n",
       "    <source src=\"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\" type=\"video/mp4\">\n",
       "    Your browser does not support the video tag.\n",
       "</video>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML\n",
    "\n",
    "video_url = \"https://d36m44n9vdbmda.cloudfront.net/assets/s-ac-04-v1/task2/NVPROF_UM_2.mp4\"\n",
    "\n",
    "video_html = f\"\"\"\n",
    "<video controls width=\"640\" height=\"360\">\n",
    "    <source src=\"{video_url}\" type=\"video/mp4\">\n",
    "    Your browser does not support the video tag.\n",
    "</video>\n",
    "\"\"\"\n",
    "\n",
    "display(HTML(video_html))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Unified Memory Migration\n",
    "\n",
    "When UM is allocated, the memory is not resident yet on either the host or the device. When either the host or device attempts to access the memory, a [page fault](https://en.wikipedia.org/wiki/Page_fault) will occur, at which point the host or device will migrate the needed data in batches. Similarly, at any point when the CPU, or any GPU in the accelerated system, attempts to access memory not yet resident on it, page faults will occur and trigger its migration.\n",
    "\n",
    "The ability to page fault and migrate memory on demand is tremendously helpful for ease of development in your accelerated applications. Additionally, when working with data that exhibits sparse access patterns, for example when it is impossible to know which data will be required to be worked on until the application actually runs, and for scenarios when data might be accessed by multiple GPU devices in an accelerated system with multiple GPUs, on-demand memory migration is remarkably beneficial.\n",
    "\n",
    "There are times - for example when data needs are known prior to runtime, and large contiguous blocks of memory are required - when the overhead of page faulting and migrating data on demand incurs an overhead cost that would be better avoided.\n",
    "\n",
    "Much of the remainder of this lab will be dedicated to understanding on-demand migration, and how to identify it in the profiler's output. With this knowledge you will be able to reduce the overhead of it in scenarios when it would be beneficial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Explore UM Migration and Page Faulting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`nsys profile` provides output describing UM behavior for the profiled application. In this exercise, you will make several modifications to a simple application, and make use of `nsys profile` after each change, to explore how UM data migration behaves.\n",
    "\n",
    "[01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) contains a `hostFunction` and a `gpuKernel`, both which could be used to initialize the elements of a `2<<24` element vector with the number `1`. Currently neither the host function nor GPU kernel are being used.\n",
    "\n",
    "For each of the 4 questions below, given what you have just learned about UM behavior, first hypothesize about what kind of page faulting should happen, then, edit [01-page-faults.cu](06-unified-memory-page-faults/01-page-faults.cu) to create a scenario, by using one or both of the 2 provided functions in the code bases, that will allow you to test your hypothesis.\n",
    "\n",
    "In order to test your hypotheses, compile and profile your code using the code execution cells below. Be sure to record your hypotheses, as well as the results, obtained from `nsys profile --stats=true` output. In the output of `nsys profile --stats=true` you should be looking for the following:\n",
    "\n",
    "- Is there a _CUDA Memory Operation Statistics_ section in the output?\n",
    "- If so, does it indicate host to device (HtoD) or device to host (DtoH) migrations?\n",
    "- When there are migrations, what does the output say about how many _Operations_ there were? If you see many small memory migration operations, this is a sign that on-demand page faulting is occurring, with small memory migrations occurring each time there is a page fault in the requested location."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the scenarios for you to explore, along with solutions for them if you get stuck:\n",
    "\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the CPU? ([solution](06-unified-memory-page-faults/solutions/01-page-faults-solution-cpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed only by the GPU? ([solution](06-unified-memory-page-faults/solutions/02-page-faults-solution-gpu-only.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the CPU then the GPU? ([solution](06-unified-memory-page-faults/solutions/03-page-faults-solution-cpu-then-gpu.cu))\n",
    "- Is there evidence of memory migration and/or page faulting when unified memory is accessed first by the GPU then the CPU? ([solution](06-unified-memory-page-faults/solutions/04-page-faults-solution-gpu-then-cpu.cu))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!nvcc -o page-faults 06-unified-memory-page-faults/01-page-faults.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating '/tmp/nsys-report-ef24.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     82.7        532228507         37  14384554.2  10069381.0      2090  100125709   22405930.9  poll                  \n",
      "     11.5         73852853         33   2237965.2   2066242.0       130   20424100    3721353.2  sem_timedwait         \n",
      "      4.8         30963244        482     64239.1     11635.0       370    8069811     388637.7  ioctl                 \n",
      "      0.7          4475733         18    248651.8      5910.0       980    4346519    1022718.4  mmap                  \n",
      "      0.1           871940         27     32294.1      3680.0      2790     545264     103264.4  mmap64                \n",
      "      0.1           622885         44     14156.5     10625.0      5000      72302      12724.5  open64                \n",
      "      0.0           201164         29      6936.7      5990.0      1590      36401       6775.5  fopen                 \n",
      "      0.0           177206          4     44301.5     39396.5     35141      63272      13212.1  pthread_create        \n",
      "      0.0           132813         11     12073.9     13360.0       820      22231       6219.2  write                 \n",
      "      0.0            57762          7      8251.7      3850.0      2560      20721       8325.2  munmap                \n",
      "      0.0            51301         26      1973.1        70.0        60      49301       9653.1  fgets                 \n",
      "      0.0            44140          6      7356.7      8850.0      3520       9540       2867.1  open                  \n",
      "      0.0            37142         22      1688.3      1795.0       670       3780        724.8  fclose                \n",
      "      0.0            34571         52       664.8       505.0       150       5030        687.8  fcntl                 \n",
      "      0.0            20531         14      1466.5      1205.0       490       4220       1138.9  read                  \n",
      "      0.0            17880          2      8940.0      8940.0      4450      13430       6349.8  socket                \n",
      "      0.0            12871          1     12871.0     12871.0     12871      12871          0.0  connect               \n",
      "      0.0             6840          5      1368.0       880.0        80       3010       1374.3  fread                 \n",
      "      0.0             6801          1      6801.0      6801.0      6801       6801          0.0  pipe2                 \n",
      "      0.0             5960         64        93.1       110.0        40        300         51.8  pthread_mutex_trylock \n",
      "      0.0             2570          1      2570.0      2570.0      2570       2570          0.0  bind                  \n",
      "      0.0             1240          1      1240.0      1240.0      1240       1240          0.0  listen                \n",
      "      0.0              200          1       200.0       200.0       200        200          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ---------------------\n",
      "     68.3        239659631          1  239659631.0  239659631.0  239659631  239659631          0.0  cudaDeviceSynchronize\n",
      "     30.4        106877497          1  106877497.0  106877497.0  106877497  106877497          0.0  cudaMallocManaged    \n",
      "      1.3          4468071          1    4468071.0    4468071.0    4468071    4468071          0.0  cudaFree             \n",
      "      0.0            25981          1      25981.0      25981.0      25981      25981          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)     Med (ns)    Min (ns)   Max (ns)   StdDev (ns)            Name          \n",
      " --------  ---------------  ---------  -----------  -----------  ---------  ---------  -----------  ------------------------\n",
      "    100.0        239643378          1  239643378.0  239643378.0  239643378  239643378          0.0  deviceKernel(int *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain GPU memory data.\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain GPU memory data.\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./page-faults"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Revisit UM Behavior for Vector Add Program\n",
    "\n",
    "Returning to the [01-vector-add.cu](01-vector-add/01-vector-add.cu) program you have been working on throughout this lab, review the code bases in its current state, and hypothesize about what kinds of memory migrations and/or page faults you expect to occur. Look at the profiling output for your last refactor (either by scrolling up to find the output or by executing the code execution cell just below), observing the _CUDA Memory Operation Statistics_ section of the profiler output. Can you explain the kinds of migrations and the number of their operations based on the contents of the code base?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-6560.qdstrm'\n",
      "[1/8] [========================100%] report3.nsys-rep\n",
      "[2/8] [========================100%] report3.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report3.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     86.9       1776023352         99  17939629.8  10072830.0      2350  100148822   26418841.3  poll                  \n",
      "      9.5        193757568         89   2177051.3   2069374.0       220   20529400    2498523.0  sem_timedwait         \n",
      "      2.5         51420908        497    103462.6     14440.0       490   13165489     734051.4  ioctl                 \n",
      "      1.0         19837612         24    826567.2      8115.5       940    7246037    2220898.5  mmap                  \n",
      "      0.1          1113079         27     41225.1      4200.0      3430     712119     135106.5  mmap64                \n",
      "      0.0           553757         44     12585.4     11355.0      4110      37721       6270.7  open64                \n",
      "      0.0           238184         29      8213.2      3840.0      1520      89793      16234.6  fopen                 \n",
      "      0.0           155094          4     38773.5     37841.0     25571      53841      12975.1  pthread_create        \n",
      "      0.0           122813         11     11164.8     10980.0       920      18841       5725.9  write                 \n",
      "      0.0            62811         11      5710.1      4060.0      1970      22081       5797.8  munmap                \n",
      "      0.0            59061         26      2271.6        90.0        70      56831      11128.0  fgets                 \n",
      "      0.0            42290          6      7048.3      7970.0      3470       9540       2513.5  open                  \n",
      "      0.0            39331         52       756.4       520.0       210       7810       1064.6  fcntl                 \n",
      "      0.0            29730         22      1351.4      1275.0       680       3390        603.6  fclose                \n",
      "      0.0            21770         14      1555.0      1410.0       490       4000       1077.9  read                  \n",
      "      0.0            17550          2      8775.0      8775.0      4730      12820       5720.5  socket                \n",
      "      0.0            12141          1     12141.0     12141.0     12141      12141          0.0  connect               \n",
      "      0.0            11310          5      2262.0      1570.0        80       5880       2397.3  fread                 \n",
      "      0.0             6310          1      6310.0      6310.0      6310       6310          0.0  pipe2                 \n",
      "      0.0             6040         64        94.4        50.0        40        480         72.8  pthread_mutex_trylock \n",
      "      0.0             2200          1      2200.0      2200.0      2200       2200          0.0  bind                  \n",
      "      0.0             1250          1      1250.0      1250.0      1250       1250          0.0  listen                \n",
      "      0.0              340          1       340.0       340.0       340        340          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     57.2        137756292          3  45918764.0     60442.0     21110  137674740   79463008.6  cudaMallocManaged    \n",
      "     34.6         83290237          1  83290237.0  83290237.0  83290237   83290237          0.0  cudaDeviceSynchronize\n",
      "      8.2         19848982          3   6616327.3   6595980.0   5977174    7275828     649566.1  cudaFree             \n",
      "      0.0            51662          1     51662.0     51662.0     51662      51662          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "    100.0         83279790          1  83279790.0  83279790.0  83279790  83279790          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     81.0         47172935  10743    4391.0    2175.0      1790     73858       8652.6  [CUDA Unified Memory memcpy HtoD]\n",
      "     19.0         11056039    768   14395.9    3727.5      1375     80545      22787.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    402.653  10743     0.037     0.008     0.004     0.963        0.116  [CUDA Unified Memory memcpy HtoD]\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report3.nsys-rep\n",
      "    /dli/task/report3.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./sm-optimized-vector-add"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Initialize Vector in Kernel\n",
    "\n",
    "When `nsys profile` gives the amount of time that a kernel takes to execute, the host-to-device page faults and data migrations that occur during this kernel's execution are included in the displayed execution time.\n",
    "\n",
    "With this in mind, refactor the `initWith` host function in your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program to instead be a CUDA kernel, initializing the allocated vector in parallel on the GPU. After successfully compiling and running the refactored application, but before profiling it, hypothesize about the following:\n",
    "\n",
    "- How do you expect the refactor to affect UM memory migration behavior?\n",
    "- How do you expect the refactor to affect the reported run time of `addVectorsInto`?\n",
    "\n",
    "Once again, record the results. Refer to [the solution](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o initialize-in-kernel 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-ebb1.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     81.0        502055502         34  14766338.3  10070724.0      2060  100139631   23224245.1  poll                  \n",
      "     10.8         66725240         30   2224174.7   2066198.5       160   20477529    3960072.9  sem_timedwait         \n",
      "      5.3         32702228        497     65799.3     11870.0       380    8185071     411918.2  ioctl                 \n",
      "      2.6         16138255         24    672427.3      4300.0      1140    7466097    1881767.5  mmap                  \n",
      "      0.2          1118297         27     41418.4      4510.0      3160     769635     146127.9  mmap64                \n",
      "      0.1           496128         44     11275.6     10525.5      4500      30421       4224.3  open64                \n",
      "      0.0           209048          4     52262.0     53501.5     38152      63893      13082.2  pthread_create        \n",
      "      0.0           185487         29      6396.1      4481.0      1380      43952       8058.3  fopen                 \n",
      "      0.0           147386         11     13398.7     13330.0       800      25991       5789.1  write                 \n",
      "      0.0            68744         11      6249.5      3320.0      1490      20071       6597.1  munmap                \n",
      "      0.0            58413         26      2246.7        90.0        70      56192      11002.7  fgets                 \n",
      "      0.0            45371          6      7561.8      8560.0      3310       9940       2494.3  open                  \n",
      "      0.0            34020         52       654.2       470.0       150       5230        717.7  fcntl                 \n",
      "      0.0            32392         22      1472.4      1325.0       700       3660        662.0  fclose                \n",
      "      0.0            22032         14      1573.7      1155.0       870       4410       1096.3  read                  \n",
      "      0.0            16280          2      8140.0      8140.0      3950      12330       5925.6  socket                \n",
      "      0.0            11861          1     11861.0     11861.0     11861      11861          0.0  connect               \n",
      "      0.0             7330          1      7330.0      7330.0      7330       7330          0.0  pipe2                 \n",
      "      0.0             7300          5      1460.0      1380.0        80       2700       1048.5  fread                 \n",
      "      0.0             5570         64        87.0        50.0        40        460         65.1  pthread_mutex_trylock \n",
      "      0.0             2421          1      2421.0      2421.0      2421       2421          0.0  bind                  \n",
      "      0.0             1150          1      1150.0      1150.0      1150       1150          0.0  listen                \n",
      "      0.0              370          1       370.0       370.0       370        370          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     67.1        109474862          3  36491620.7     31771.0     16641  109426450   63163415.5  cudaMallocManaged    \n",
      "     23.0         37486693          1  37486693.0  37486693.0  37486693   37486693          0.0  cudaDeviceSynchronize\n",
      "      9.9         16194467          3   5398155.7   4394795.0   4301763    7497909    1819034.6  cudaFree             \n",
      "      0.0            56641          4     14160.3      7140.0      5130      37231      15495.9  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     97.5         36569157          3  12189719.0  12104245.0  12103285  12361627     148877.5  initWith_GPU(float, float *, int)             \n",
      "      2.5           933974          1    933974.0    933974.0    933974    933974          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11076264    768   14422.2    3808.0      1439     80578      22788.7  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./initialize-in-kernel"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Asynchronous Memory Prefetching\n",
    "\n",
    "A powerful technique to reduce the overhead of page faulting and on-demand memory migrations, both in host-to-device and device-to-host memory transfers, is called **asynchronous memory prefetching**. Using this technique allows programmers to asynchronously migrate unified memory (UM) to any CPU or GPU device in the system, in the background, prior to its use by application code. By doing this, GPU kernels and CPU function performance can be increased on account of reduced page fault and on-demand data migration overhead.\n",
    "\n",
    "Prefetching also tends to migrate data in larger chunks, and therefore fewer trips, than on-demand migration. This makes it an excellent fit when data access needs are known before runtime, and when data access patterns are not sparse.\n",
    "\n",
    "CUDA Makes asynchronously prefetching managed memory to either a GPU device or the CPU easy with its `cudaMemPrefetchAsync` function. Here is an example of using it to both prefetch data to the currently active GPU device, and then, to the CPU:\n",
    "\n",
    "```cpp\n",
    "int deviceId;\n",
    "cudaGetDevice(&deviceId);                                         // The ID of the currently active GPU device.\n",
    "\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, deviceId);        // Prefetch to GPU device.\n",
    "cudaMemPrefetchAsync(pointerToSomeUMData, size, cudaCpuDeviceId); // Prefetch to host. `cudaCpuDeviceId` is a\n",
    "                                                                  // built-in CUDA variable.\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory\n",
    "\n",
    "At this point in the lab, your [01-vector-add.cu](01-vector-add/01-vector-add.cu) program should not only be launching a CUDA kernel to add 2 vectors into a third solution vector, all which are allocated with `cudaMallocManaged`, but should also be initializing each of the 3 vectors in parallel in a CUDA kernel. If for some reason, your application does not do any of the above, please refer to the following [reference application](07-init-in-kernel/solutions/01-vector-add-init-in-kernel-solution.cu), and update your own code bases to reflect its current functionality.\n",
    "\n",
    "Conduct 3 experiments using `cudaMemPrefetchAsync` inside of your [01-vector-add.cu](01-vector-add/01-vector-add.cu) application to understand its impact on page-faulting and memory migration.\n",
    "\n",
    "- What happens when you prefetch one of the initialized vectors to the device?\n",
    "- What happens when you prefetch two of the initialized vectors to the device?\n",
    "- What happens when you prefetch all three of the initialized vectors to the device?\n",
    "\n",
    "Hypothesize about UM behavior, page faulting specifically, as well as the impact on the reported run time of the initialization kernel, before each experiment, and then verify by running `nsys profile`. Refer to [the solution](08-prefetch/solutions/01-vector-add-prefetch-solution.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-gpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-de67.qdstrm'\n",
      "[1/8] [========================100%] report4.nsys-rep\n",
      "[2/8] [========================100%] report4.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report4.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     79.7        491332277         33  14888856.9  10067611.0      2210  100134566   23288302.8  poll                  \n",
      "     11.0         67752617         29   2336297.1   2064375.0       160   20526606    4217587.6  sem_timedwait         \n",
      "      6.3         38986972        497     78444.6     12990.0       370    9567270     529699.6  ioctl                 \n",
      "      2.6         16087839         24    670326.6      5419.5      1160    7389856    1870690.2  mmap                  \n",
      "      0.2          1233818         27     45697.0      4660.0      3610     839616     159400.2  mmap64                \n",
      "      0.1           557484         44     12670.1     12255.0      4560      39509       6114.7  open64                \n",
      "      0.0           191859          4     47964.8     45720.0     39810      60609       9764.8  pthread_create        \n",
      "      0.0           160419         11     14583.5     16020.0      1170      25270       6218.6  write                 \n",
      "      0.0           156724         29      5404.3      3970.0      1570      22500       4788.6  fopen                 \n",
      "      0.0            61199         26      2353.8        90.0        70      58929      11539.1  fgets                 \n",
      "      0.0            58289         11      5299.0      4010.0      1480      13099       3608.0  munmap                \n",
      "      0.0            45029          6      7504.8      5095.0      3750      17200       5144.6  open                  \n",
      "      0.0            35139         52       675.8       530.0       150       6310        847.2  fcntl                 \n",
      "      0.0            28730         22      1305.9      1145.0       620       3980        690.3  fclose                \n",
      "      0.0            19120         14      1365.7      1175.0       410       3880       1008.3  read                  \n",
      "      0.0            16310          2      8155.0      8155.0      4470      11840       5211.4  socket                \n",
      "      0.0            10320          1     10320.0     10320.0     10320      10320          0.0  connect               \n",
      "      0.0            10089          5      2017.8      1710.0        90       3629       1469.9  fread                 \n",
      "      0.0             7670          1      7670.0      7670.0      7670       7670          0.0  pipe2                 \n",
      "      0.0             5750         64        89.8        80.0        40        190         48.6  pthread_mutex_trylock \n",
      "      0.0             2900          1      2900.0      2900.0      2900       2900          0.0  bind                  \n",
      "      0.0             1240          1      1240.0      1240.0      1240       1240          0.0  listen                \n",
      "      0.0              290          1       290.0       290.0       290        290          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     70.0        116663789          3  38887929.7     41479.0     17780  116604530   67304551.2  cudaMallocManaged    \n",
      "     20.2         33725076          1  33725076.0  33725076.0  33725076   33725076          0.0  cudaDeviceSynchronize\n",
      "      9.7         16113280          3   5371093.3   4384896.0   4299428    7428956    1782673.6  cudaFree             \n",
      "      0.0            62539          4     15634.8      6875.0      5250      43539      18638.9  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     97.2         32795041          3  10931680.3  11054130.0  10279143  11461768     600746.1  initWith_GPU(float, float *, int)             \n",
      "      2.8           932902          1    932902.0    932902.0    932902    932902          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11078324    768   14424.9    3775.5      1407     80766      22787.1  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report4.nsys-rep\n",
      "    /dli/task/report4.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Prefetch Memory Back to the CPU\n",
    "\n",
    "Add additional prefetching back to the CPU for the function that verifies the correctness of the `addVectorInto` kernel. Again, hypothesize about the impact on UM before profiling in `nsys` to confirm. Refer to [the solution](08-prefetch/solutions/02-vector-add-prefetch-solution-cpu-also.cu) if you get stuck."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n"
     ]
    }
   ],
   "source": [
    "!nvcc -o prefetch-to-cpu 01-vector-add/01-vector-add.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Success! All values calculated correctly.\n",
      "Generating '/tmp/nsys-report-a020.qdstrm'\n",
      "[1/8] [========================100%] report5.nsys-rep\n",
      "[2/8] [========================100%] report5.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report5.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     79.2        501377915         34  14746409.3  10070074.0      2300  100132087   23042087.4  poll                  \n",
      "     12.1         76321876         30   2544062.5   2064147.0       130   20504431    4926666.1  sem_timedwait         \n",
      "      5.9         37035517        497     74518.1     12980.0       370    8113321     461908.6  ioctl                 \n",
      "      2.5         16002114         24    666754.8      5945.0       880    7326553    1857818.4  mmap                  \n",
      "      0.1           904712         27     33507.9      4360.0      3200     557031     105539.7  mmap64                \n",
      "      0.1           565142         44     12844.1     11030.0      4360      41109       7106.4  open64                \n",
      "      0.0           199179         29      6868.2      4970.0      1200      39329       7339.9  fopen                 \n",
      "      0.0           187187          4     46796.8     39834.5     35079      72439      17636.0  pthread_create        \n",
      "      0.0           158327         11     14393.4     15130.0      1160      24189       5860.1  write                 \n",
      "      0.0            66968         12      5580.7      3890.0      1380      19170       4946.4  munmap                \n",
      "      0.0            62099         26      2388.4       100.0        70      59579      11664.7  fgets                 \n",
      "      0.0            44000          6      7333.3      8040.0      3910      10550       2819.1  open                  \n",
      "      0.0            36639         22      1665.4      1655.0       750       3430        693.5  fclose                \n",
      "      0.0            35280         52       678.5       530.0       220       5430        716.1  fcntl                 \n",
      "      0.0            22119         14      1579.9      1189.5       520       4290       1113.1  read                  \n",
      "      0.0            16630          2      8315.0      8315.0      4230      12400       5777.1  socket                \n",
      "      0.0            12660          1     12660.0     12660.0     12660      12660          0.0  connect               \n",
      "      0.0             7400          1      7400.0      7400.0      7400       7400          0.0  pipe2                 \n",
      "      0.0             7350         64       114.8       130.0        40        490         83.9  pthread_mutex_trylock \n",
      "      0.0             7110          5      1422.0      1240.0        60       2570       1019.3  fread                 \n",
      "      0.0             2510          1      2510.0      2510.0      2510       2510          0.0  bind                  \n",
      "      0.0             1450          1      1450.0      1450.0      1450       1450          0.0  listen                \n",
      "      0.0              440          1       440.0       440.0       440        440          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ---------------------\n",
      "     70.6        114124783          3  38041594.3     79399.0     19879  114025505   65804003.6  cudaMallocManaged    \n",
      "     19.5         31503856          1  31503856.0  31503856.0  31503856   31503856          0.0  cudaDeviceSynchronize\n",
      "      9.9         16003073          3   5334357.7   4341210.0   4309801    7352062    1747453.8  cudaFree             \n",
      "      0.0            58419          4     14604.8      7125.0      5599      38570      15998.1  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)                       Name                     \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------------------------------\n",
      "     97.0         30571699          3  10190566.3  10146322.0   9966775  10458602     248880.7  initWith_GPU(float, float *, int)             \n",
      "      3.0           940167          1    940167.0    940167.0    940167    940167          0.0  addVectorsInto(float *, float *, float *, int)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0         11083199    768   14431.2    3791.0      1439     80766      22783.5  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    134.218    768     0.175     0.033     0.004     1.044        0.301  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report5.nsys-rep\n",
      "    /dli/task/report5.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./prefetch-to-cpu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this series of refactors to use asynchronous prefetching, you should see that there are fewer, but larger, memory transfers, and, that the kernel execution time is significantly decreased."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Summary\n",
    "\n",
    "At this point in the lab, you are able to:\n",
    "\n",
    "- Use the Nsight Systems command line tool (**nsys**) to profile accelerated application performance.\n",
    "- Leverage an understanding of **Streaming Multiprocessors** to optimize execution configurations.\n",
    "- Understand the behavior of **Unified Memory** with regard to page faulting and data migrations.\n",
    "- Use **asynchronous memory prefetching** to reduce page faults and data migrations for increased performance.\n",
    "- Employ an iterative development cycle to rapidly accelerate and deploy applications.\n",
    "\n",
    "In order to consolidate your learning, and reinforce your ability to iteratively accelerate, optimize, and deploy applications, please proceed to this lab's final exercise. After completing it, for those of you with time and interest, please proceed to the *Advanced Content* section."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Final Exercise: Iteratively Optimize an Accelerated SAXPY Application\n",
    "\n",
    "A basic accelerated SAXPY (Single Precision a\\*x+b) application has been provided for you [here](09-saxpy/01-saxpy.cu). It currently works and you can compile, run, and then profile it with `nsys profile` below.\n",
    "\n",
    "Record the runtime of the `saxpy` kernel without making any modifications and then work *iteratively* to optimize the application, using `nsys profile` after each iteration to notice the effects of the code changes on kernel performance and UM behavior.\n",
    "\n",
    "Utilize the techniques from this lab. To support your learning, utilize [effortful retrieval](http://sites.gsu.edu/scholarlyteaching/effortful-retrieval/) whenever possible, rather than rushing to look up the specifics of techniques from earlier in the lesson.\n",
    "\n",
    "Your end goal is to profile an accurate `saxpy` kernel, without modifying `N`, to run in under *200,000 ns*. Check out [the solution](09-saxpy/solutions/02-saxpy-solution.cu) if you get stuck, and feel free to compile and profile it if you wish."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n"
     ]
    }
   ],
   "source": [
    "!nvcc -o saxpy 09-saxpy/01-saxpy.cu -run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 5, c[1] = 5, c[2] = 5, c[3] = 5, c[4] = 5, \n",
      "c[4194299] = 5, c[4194300] = 5, c[4194301] = 5, c[4194302] = 5, c[4194303] = 5, \n",
      "Generating '/tmp/nsys-report-8272.qdstrm'\n",
      "[1/8] [========================100%] report2.nsys-rep\n",
      "[2/8] [========================100%] report2.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report2.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  ----------------------\n",
      "     74.1        266268105         25  10650724.2  10068793.0      2290  73969015   16373641.3  poll                  \n",
      "     13.7         49080341         19   2583175.8     56541.0       120  20476811    5552380.5  sem_timedwait         \n",
      "     10.8         38880567        500     77761.1     12750.0       380   8227365     439034.2  ioctl                 \n",
      "      0.6          2241224         23     97444.5      6240.0      1250    738991     238407.2  mmap                  \n",
      "      0.3           931954         27     34516.8      4640.0      3000    580779     110012.5  mmap64                \n",
      "      0.1           537196         44     12209.0     11515.0      3000     31231       5014.5  open64                \n",
      "      0.1           499918          3    166639.3    197943.0     93661    208314      63413.5  sem_wait              \n",
      "      0.1           309856          5     61971.2     51711.0     37480    106172      28830.4  pthread_create        \n",
      "      0.1           183893         29      6341.1      3560.0       890     44680       8686.0  fopen                 \n",
      "      0.0           175782         13     13521.7     14791.0      3810     17880       4123.8  write                 \n",
      "      0.0            50810         26      1954.2        70.0        60     49040       9603.7  fgets                 \n",
      "      0.0            41060          6      6843.3      7530.0      2480      9640       2597.6  open                  \n",
      "      0.0            35190         52       676.7       530.0       160      5360        731.5  fcntl                 \n",
      "      0.0            30760          9      3417.8      3300.0      1390      6430       1615.0  munmap                \n",
      "      0.0            27981         22      1271.9      1050.0       490      3450        677.1  fclose                \n",
      "      0.0            22090         16      1380.6      1140.0       470      4170       1055.8  read                  \n",
      "      0.0            16270          5      3254.0      1390.0       110      9920       4096.6  fread                 \n",
      "      0.0            13701          2      6850.5      6850.5      4961      8740       2672.2  socket                \n",
      "      0.0             8890          1      8890.0      8890.0      8890      8890          0.0  connect               \n",
      "      0.0             6610          1      6610.0      6610.0      6610      6610          0.0  pipe2                 \n",
      "      0.0             5890         64        92.0        50.0        40       360         57.4  pthread_mutex_trylock \n",
      "      0.0             2410          1      2410.0      2410.0      2410      2410          0.0  bind                  \n",
      "      0.0             1320          1      1320.0      1320.0      1320      1320          0.0  listen                \n",
      "      0.0              170          1       170.0       170.0       170       170          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)          Name         \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  ---------------------\n",
      "     93.7        112980888          3  37660296.0    20701.0     18310  112941877   65195761.6  cudaMallocManaged    \n",
      "      3.1          3746727          1   3746727.0  3746727.0   3746727    3746727          0.0  cudaDeviceSynchronize\n",
      "      1.8          2187684          3    729228.0   706881.0    700291     780512      44535.3  cudaFree             \n",
      "      1.3          1580954          3    526984.7   249454.0      8450    1323050     699864.8  cudaMemPrefetchAsync \n",
      "      0.0            34830          1     34830.0    34830.0     34830      34830          0.0  cudaLaunchKernel     \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  --------  --------  --------  --------  -----------  --------------------------\n",
      "    100.0           104321          1  104321.0  104321.0    104321    104321          0.0  saxpy(int *, int *, int *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     99.6          3815825     24  158992.7  158769.0    158689    160961        475.3  [CUDA Unified Memory memcpy HtoD]\n",
      "      0.4            14430      4    3607.5    3599.5      1407      5824       2485.8  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     50.332     24     2.097     2.097     2.097     2.097        0.000  [CUDA Unified Memory memcpy HtoD]\n",
      "      0.131      4     0.033     0.033     0.004     0.061        0.033  [CUDA Unified Memory memcpy DtoH]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report2.nsys-rep\n",
      "    /dli/task/report2.sqlite\n"
     ]
    }
   ],
   "source": [
    "!nsys profile --stats=true ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c[0] = 0, c[1] = 0, c[2] = 0, c[3] = 0, c[4] = 0, \n",
      "c[4194299] = 0, c[4194300] = 0, c[4194301] = 0, c[4194302] = 0, c[4194303] = 0, \n",
      "Generating '/tmp/nsys-report-0cb9.qdstrm'\n",
      "[1/8] [========================100%] report.nsys-rep\n",
      "[2/8] [========================100%] report.sqlite\n",
      "[3/8] Executing 'nvtx_sum' stats report\n",
      "SKIPPED: /dli/task/report.sqlite does not contain NV Tools Extension (NVTX) data.\n",
      "[4/8] Executing 'osrt_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)    Med (ns)   Min (ns)  Max (ns)   StdDev (ns)           Name         \n",
      " --------  ---------------  ---------  ----------  ----------  --------  ---------  -----------  ----------------------\n",
      "     73.7        301090224         23  13090879.3  10070277.0      2060  100131020   20819931.1  poll                  \n",
      "     13.5         55072398         20   2753619.9    201241.5       210   20536346    5966991.8  sem_timedwait         \n",
      "     11.5         46911638        497     94389.6     11650.0       390   14153527     743799.8  ioctl                 \n",
      "      0.8          3251525         23    141370.7      6040.0       910    1050925     357603.3  mmap                  \n",
      "      0.2           933257         27     34565.1      4290.0      3060     603833     114423.8  mmap64                \n",
      "      0.1           504682         44     11470.0     10105.0      3200      49391       7295.7  open64                \n",
      "      0.0           165853          4     41463.3     36345.5     32181      60981      13237.7  pthread_create        \n",
      "      0.0           155320         29      5355.9      3470.0      1460      30430       5973.6  fopen                 \n",
      "      0.0           149140         11     13558.2     14390.0       890      22780       5762.7  write                 \n",
      "      0.0            50590         26      1945.8        70.0        60      48710       9538.1  fgets                 \n",
      "      0.0            44500         10      4450.0      2995.0      1390      19610       5416.2  munmap                \n",
      "      0.0            34730          6      5788.3      6280.0      2460       8370       2283.5  open                  \n",
      "      0.0            31600         52       607.7       440.0       150       4870        673.4  fcntl                 \n",
      "      0.0            24770         22      1125.9      1010.0       530       3320        585.6  fclose                \n",
      "      0.0            18180         14      1298.6       970.0       520       3380        939.3  read                  \n",
      "      0.0            14890          2      7445.0      7445.0      3090      11800       6158.9  socket                \n",
      "      0.0            11400          1     11400.0     11400.0     11400      11400          0.0  connect               \n",
      "      0.0             7360          5      1472.0       250.0        80       3670       1792.3  fread                 \n",
      "      0.0             6320          1      6320.0      6320.0      6320       6320          0.0  pipe2                 \n",
      "      0.0             6030         64        94.2        60.0        50        180         47.6  pthread_mutex_trylock \n",
      "      0.0             2270          1      2270.0      2270.0      2270       2270          0.0  bind                  \n",
      "      0.0             1110          1      1110.0      1110.0      1110       1110          0.0  listen                \n",
      "      0.0              290          1       290.0       290.0       290        290          0.0  pthread_cond_broadcast\n",
      "\n",
      "[5/8] Executing 'cuda_api_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Num Calls   Avg (ns)   Med (ns)   Min (ns)  Max (ns)   StdDev (ns)        Name       \n",
      " --------  ---------------  ---------  ----------  ---------  --------  ---------  -----------  -----------------\n",
      "     89.6        128489914          3  42829971.3    21501.0     17490  128450923   74149919.3  cudaMallocManaged\n",
      "     10.4         14898420          3   4966140.0  1074695.0   1071795   12751930    6742692.1  cudaFree         \n",
      "      0.0            51120          1     51120.0    51120.0     51120      51120          0.0  cudaLaunchKernel \n",
      "\n",
      "[6/8] Executing 'cuda_gpu_kern_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Instances   Avg (ns)    Med (ns)   Min (ns)  Max (ns)  StdDev (ns)             Name           \n",
      " --------  ---------------  ---------  ----------  ----------  --------  --------  -----------  --------------------------\n",
      "    100.0         11664860          1  11664860.0  11664860.0  11664860  11664860          0.0  saxpy(int *, int *, int *)\n",
      "\n",
      "[7/8] Executing 'cuda_gpu_mem_time_sum' stats report\n",
      "\n",
      " Time (%)  Total Time (ns)  Count  Avg (ns)  Med (ns)  Min (ns)  Max (ns)  StdDev (ns)              Operation            \n",
      " --------  ---------------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "    100.0          8095845   2616    3094.7    2175.0      1854     73375       3215.9  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "[8/8] Executing 'cuda_gpu_mem_size_sum' stats report\n",
      "\n",
      " Total (MB)  Count  Avg (MB)  Med (MB)  Min (MB)  Max (MB)  StdDev (MB)              Operation            \n",
      " ----------  -----  --------  --------  --------  --------  -----------  ---------------------------------\n",
      "     50.332   2616     0.019     0.008     0.004     0.954        0.043  [CUDA Unified Memory memcpy HtoD]\n",
      "\n",
      "Generated:\n",
      "    /dli/task/report.nsys-rep\n",
      "    /dli/task/report.sqlite\n",
      "ERROR: Specified input file (report.qdstrm) does not exist.\n",
      "\n",
      "usage: nsys stats [<args>] <input-file>\n",
      "Try 'nsys stats --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "# Profile the application\n",
    "!nsys profile --stats=true -o report ./saxpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "WARNING: Existing SQLite export found: report.sqlite\n",
      "         File is older than input file: report.nsys-rep\n",
      "         Use --force-export=true to update export file.\n",
      "\n",
      "usage: nsys stats [<args>] <input-file>\n",
      "Try 'nsys stats --help' for more information.\n"
     ]
    }
   ],
   "source": [
    "# Generate a summary report\n",
    "!nsys stats --report summary report.nsys-rep"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Get:1 http://security.ubuntu.com/ubuntu focal-security InRelease [114 kB]\n",
      "Get:2 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  InRelease [1581 B]\n",
      "Get:3 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2004/x86_64  Packages [1520 kB]\n",
      "Hit:4 http://archive.ubuntu.com/ubuntu focal InRelease  \n",
      "Get:5 http://security.ubuntu.com/ubuntu focal-security/main amd64 Packages [3670 kB]\n",
      "Get:6 http://archive.ubuntu.com/ubuntu focal-updates InRelease [114 kB]        \n",
      "Get:7 http://security.ubuntu.com/ubuntu focal-security/multiverse amd64 Packages [29.8 kB]\n",
      "Get:8 http://security.ubuntu.com/ubuntu focal-security/universe amd64 Packages [1205 kB]\n",
      "Get:9 http://security.ubuntu.com/ubuntu focal-security/restricted amd64 Packages [3613 kB]\n",
      "Get:10 http://archive.ubuntu.com/ubuntu focal-backports InRelease [108 kB]     \n",
      "Get:11 http://archive.ubuntu.com/ubuntu focal-updates/restricted amd64 Packages [3762 kB]\n",
      "Get:12 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 Packages [4145 kB]\n",
      "Get:13 http://archive.ubuntu.com/ubuntu focal-updates/universe amd64 Packages [1502 kB]\n",
      "Get:14 http://archive.ubuntu.com/ubuntu focal-updates/multiverse amd64 Packages [32.5 kB]\n",
      "Fetched 19.8 MB in 2s (13.2 MB/s)                             \n",
      "Reading package lists... Done\n"
     ]
    }
   ],
   "source": [
    "!apt-get update"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading package lists... Done\n",
      "Building dependency tree       \n",
      "Reading state information... Done\n",
      "The following NEW packages will be installed:\n",
      "  unzip zip\n",
      "0 upgraded, 2 newly installed, 0 to remove and 90 not upgraded.\n",
      "Need to get 336 kB of archives.\n",
      "After this operation, 1231 kB of additional disk space will be used.\n",
      "Get:1 http://archive.ubuntu.com/ubuntu focal-updates/main amd64 unzip amd64 6.0-25ubuntu1.2 [169 kB]\n",
      "Get:2 http://archive.ubuntu.com/ubuntu focal/main amd64 zip amd64 3.0-11build1 [167 kB]\n",
      "Fetched 336 kB in 0s (3613 kB/s)\n",
      "debconf: delaying package configuration, since apt-utils is not installed\n",
      "Selecting previously unselected package unzip.\n",
      "(Reading database ... 17609 files and directories currently installed.)\n",
      "Preparing to unpack .../unzip_6.0-25ubuntu1.2_amd64.deb ...\n",
      "Unpacking unzip (6.0-25ubuntu1.2) ...\n",
      "Selecting previously unselected package zip.\n",
      "Preparing to unpack .../zip_3.0-11build1_amd64.deb ...\n",
      "Unpacking zip (3.0-11build1) ...\n",
      "Setting up unzip (6.0-25ubuntu1.2) ...\n",
      "Setting up zip (3.0-11build1) ...\n"
     ]
    }
   ],
   "source": [
    "!apt-get install -y zip unzip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
